{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level Features vs Cross-Over Points\n",
    "\n",
    "Using machine learning models to predict cross-over bitrates  from low-level video features extracted from uncompressed videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import os, sys, warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity\")\n",
    "import quality_ladder_construction.QL_functions.dataset_evaluation_functions as dataset_evaluation_functions\n",
    "import defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "rq_points_dataset_path = defaults.rq_points_dataset_path\n",
    "features_path = defaults.llf_features_path\n",
    "\n",
    "# Features\n",
    "features_set = []\n",
    "for features_subset in [defaults.glcm_features, defaults.tc_features, defaults.si_features, defaults.ti_features, defaults.cti_features, defaults.cf_features, defaults.ci_features, defaults.dct_features]:\n",
    "\tfor f in features_subset:\n",
    "\t\tif \"max\" in f:\n",
    "\t\t\tcontinue\n",
    "\t\tfeatures_set.append(f)\n",
    "\t\n",
    "# Files\n",
    "train_video_filenames = defaults.Train_Video_Titles\n",
    "valid_video_filenames = defaults.Valid_Video_Titles\n",
    "test_video_filenames = defaults.Test_Video_Titles\n",
    "\n",
    "# Parameters\n",
    "Resolutions = defaults.resolutions\n",
    "CRFs = defaults.CRFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Over Quality-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Validation and Test Datasets\n",
    "X_train_1, y_train_1 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=train_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(3840,2160),\n",
    "\tlow_res=(2560,1440)\n",
    ")\n",
    "X_valid_1, y_valid_1 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tvideo_filenames=valid_video_filenames,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(3840,2160),\n",
    "\tlow_res=(2560,1440)\n",
    ")\n",
    "X_test_1, y_test_1 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=test_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(3840,2160),\n",
    "\tlow_res=(2560,1440)\n",
    ")\n",
    "\n",
    "# Assertions\n",
    "assert (X_train_1.shape[1] == X_valid_1.shape[1] == X_test_1.shape[1]) and (y_train_1.shape[1] == y_valid_1.shape[1] == y_test_1.shape[1]), \"No.of features do not match.\"\n",
    "print (\"Dimensions of training dataset = \", X_train_1.shape, y_train_1.shape)\n",
    "print (\"Dimensions of validation dataset = \", X_valid_1.shape, y_valid_1.shape)\n",
    "print (\"Dimensions of test dataset = \", X_test_1.shape, y_test_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "rfe_1 = RFE(estimator=RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\"), n_features_to_select=9)\n",
    "rfe_1.fit(X_train_1, y_train_1)\n",
    "indices = [i for i,v in enumerate(rfe_1.support_) if v]\n",
    "\n",
    "# Features Selected\n",
    "f = features_set\n",
    "print ([f[i] for i in indices])\n",
    "\n",
    "# Feature-Elimination on Inputs\n",
    "X_train_1 = X_train_1[...,indices]\n",
    "X_valid_1 = X_valid_1[...,indices]\n",
    "X_test_1 = X_test_1[...,indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extra-Trees Regressor\n",
    "# print (\"Extra-Trees Regressor:\")\n",
    "# ETR_model_1 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# ETR_model_1.fit(X_train_1, y_train_1)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(ETR_model_1.predict(X_valid_1), y_valid_1))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(ETR_model_1.predict(X_valid_1), y_valid_1)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(ETR_model_1.predict(X_valid_1), y_valid_1)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # XG-Boost\n",
    "# print (\"XG-Boost Regressor:\")\n",
    "# XGB_model_1 = XGBRegressor(n_estimators=1250, learning_rate=0.0075, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# XGB_model_1.fit(X_train_1,y_train_1)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(XGB_model_1.predict(X_valid_1), y_valid_1))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(XGB_model_1.predict(X_valid_1), y_valid_1)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(XGB_model_1.predict(X_valid_1), y_valid_1)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # Random Forest\n",
    "# print (\"Random-Forest Regressor:\")\n",
    "# RF_model_1 = RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# RF_model_1.fit(X_train_1, y_train_1)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(RF_model_1.predict(X_valid_1), y_valid_1))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(RF_model_1.predict(X_valid_1), y_valid_1)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(RF_model_1.predict(X_valid_1), y_valid_1)[0])\n",
    "# print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-training best model on best features\n",
    "test_model_1 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "test_model_1.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Validation\n",
    "print (\"Validation:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_1.predict(X_valid_1), y_valid_1), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_1.predict(X_valid_1), y_valid_1)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_1.predict(X_valid_1), y_valid_1)[0], decimals=3))\n",
    "print ()\n",
    "\n",
    "# Testing the model\n",
    "print (\"Testing:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_1.predict(X_test_1), y_test_1), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_1.predict(X_test_1), y_test_1)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_1.predict(X_test_1), y_test_1)[0], decimals=3))\n",
    "\n",
    "# Saving the weights\n",
    "pickle.dump(test_model_1, open(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/model_coq_1.pkl\", \"wb\"))\n",
    "np.save(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/feature_indices_coq_1.npy\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Over Quality-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous Cross-Over Qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_COQ_1 = test_model_1.predict(X_train_1).reshape(-1,1)\n",
    "valid_COQ_1 = test_model_1.predict(X_valid_1).reshape(-1,1)\n",
    "test_COQ_1 = test_model_1.predict(X_test_1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Validation and Test Datasets\n",
    "X_train_2, y_train_2 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=train_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(2560,1440),\n",
    "\tlow_res=(1920,1080)\n",
    ")\n",
    "X_valid_2, y_valid_2 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=valid_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(2560,1440),\n",
    "\tlow_res=(1920,1080)\n",
    ")\n",
    "X_test_2, y_test_2 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=test_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(2560,1440),\n",
    "\tlow_res=(1920,1080)\n",
    ")\n",
    "\n",
    "# Concatenating Low-Level Features and Previous Cross-Over Qualities\n",
    "X_train_2 = np.concatenate([X_train_2, train_COQ_1], axis=1)\n",
    "X_valid_2 = np.concatenate([X_valid_2, valid_COQ_1], axis=1)\n",
    "X_test_2 = np.concatenate([X_test_2, test_COQ_1], axis=1)\n",
    "\n",
    "\n",
    "# Assertions\n",
    "assert (X_train_2.shape[1] == X_valid_2.shape[1] == X_test_2.shape[1]) and (y_train_2.shape[1] == y_valid_2.shape[1] == y_test_2.shape[1]), \"No.of features do not match.\"\n",
    "print (\"Dimensions of training dataset = \", X_train_2.shape, y_train_2.shape)\n",
    "print (\"Dimensions of validation dataset = \", X_valid_2.shape, y_valid_2.shape)\n",
    "print (\"Dimensions of test dataset = \", X_test_2.shape, y_test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "rfe_2 = RFE(estimator=RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\"), n_features_to_select=9)\n",
    "rfe_2.fit(X_train_2, y_train_2)\n",
    "indices = [i for i,v in enumerate(rfe_2.support_) if v]\n",
    "\n",
    "# Features Selected\n",
    "f = features_set + [\"COQ_1\"]\n",
    "print ([f[i] for i in indices])\n",
    "\n",
    "# Feature-Elimination on Inputs\n",
    "X_train_2 = X_train_2[...,indices]\n",
    "X_valid_2 = X_valid_2[...,indices]\n",
    "X_test_2 = X_test_2[...,indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extra-Trees Regressor\n",
    "# print (\"Extra-Trees Regressor:\")\n",
    "# ETR_model_2 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# ETR_model_2.fit(X_train_2, y_train_2)\n",
    "\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(ETR_model_2.predict(X_valid_2), y_valid_2))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(ETR_model_2.predict(X_valid_2), y_valid_2)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(ETR_model_2.predict(X_valid_2), y_valid_2)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # XG-Boost\n",
    "# print (\"XG-Boost Regressor:\")\n",
    "# XGB_model_2 = XGBRegressor(n_estimators=1250, learning_rate=0.0075, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# XGB_model_2.fit(X_train_2,y_train_2)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(XGB_model_2.predict(X_valid_2), y_valid_2))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(XGB_model_2.predict(X_valid_2), y_valid_2)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(XGB_model_2.predict(X_valid_2), y_valid_2)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # Random Forest\n",
    "# print (\"Random-Forest Regressor:\")\n",
    "# RF_model_2 = RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# RF_model_2.fit(X_train_2, y_train_2)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(RF_model_2.predict(X_valid_2), y_valid_2))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(RF_model_2.predict(X_valid_2), y_valid_2)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(RF_model_2.predict(X_valid_2), y_valid_2)[0])\n",
    "# print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-training best model on best features\n",
    "test_model_2 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "test_model_2.fit(X_train_2, y_train_2)\n",
    "\n",
    "# Validation\n",
    "print (\"Validation:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_2.predict(X_valid_2), y_valid_2), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_2.predict(X_valid_2), y_valid_2)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_2.predict(X_valid_2), y_valid_2)[0], decimals=3))\n",
    "print ()\n",
    "\n",
    "# Testing the model\n",
    "print (\"Testing:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_2.predict(X_test_2), y_test_2), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_2.predict(X_test_2), y_test_2)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_2.predict(X_test_2), y_test_2)[0], decimals=3))\n",
    "\n",
    "# Saving the weights\n",
    "pickle.dump(test_model_2, open(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/model_coq_2.pkl\", \"wb\"))\n",
    "np.save(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/feature_indices_coq_2.npy\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Over Quality-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous Cross-Over Qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_COQ_2 = test_model_2.predict(X_train_2).reshape(-1,1)\n",
    "valid_COQ_2 = test_model_2.predict(X_valid_2).reshape(-1,1)\n",
    "test_COQ_2 = test_model_2.predict(X_test_2).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Validation and Test Datasets\n",
    "X_train_3, y_train_3 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=train_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(1920,1080),\n",
    "\tlow_res=(1280,720)\n",
    ")\n",
    "X_valid_3, y_valid_3 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=valid_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(1920,1080),\n",
    "\tlow_res=(1280,720)\n",
    ")\n",
    "X_test_3, y_test_3 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=test_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(1920,1080),\n",
    "\tlow_res=(1280,720)\n",
    ")\n",
    "\n",
    "# Concatenating Low-Level Features and Previous Cross-Over Qualities\n",
    "X_train_3 = np.concatenate([X_train_3, train_COQ_1, train_COQ_2], axis=1)\n",
    "X_valid_3 = np.concatenate([X_valid_3, valid_COQ_1, valid_COQ_2], axis=1)\n",
    "X_test_3 = np.concatenate([X_test_3, test_COQ_1, test_COQ_2], axis=1)\n",
    "\n",
    "\n",
    "# Assertions\n",
    "assert (X_train_3.shape[1] == X_valid_3.shape[1] == X_test_3.shape[1]) and (y_train_3.shape[1] == y_valid_3.shape[1] == y_test_3.shape[1]), \"No.of features do not match.\"\n",
    "print (\"Dimensions of training dataset = \", X_train_3.shape, y_train_3.shape)\n",
    "print (\"Dimensions of validation dataset = \", X_valid_3.shape, y_valid_3.shape)\n",
    "print (\"Dimensions of test dataset = \", X_test_3.shape, y_test_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "rfe_3 = RFE(estimator=RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\"), n_features_to_select=9)\n",
    "rfe_3.fit(X_train_3, y_train_3)\n",
    "indices = [i for i,v in enumerate(rfe_3.support_) if v]\n",
    "\n",
    "# Features Selected\n",
    "f = features_set + [\"COQ_1\", \"COQ_2\"]\n",
    "print ([f[i] for i in indices])\n",
    "\n",
    "# Feature-Elimination on Inputs\n",
    "X_train_3 = X_train_3[...,indices]\n",
    "X_valid_3 = X_valid_3[...,indices]\n",
    "X_test_3 = X_test_3[...,indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extra-Trees Regressor\n",
    "# print (\"Extra-Trees Regressor:\")\n",
    "# ETR_model_3 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# ETR_model_3.fit(X_train_3, y_train_3)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(ETR_model_3.predict(X_valid_3), y_valid_3))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(ETR_model_3.predict(X_valid_3), y_valid_3)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(ETR_model_3.predict(X_valid_3), y_valid_3)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # XG-Boost\n",
    "# print (\"XG-Boost Regressor:\")\n",
    "# XGB_model_3 = XGBRegressor(n_estimators=1250, learning_rate=0.0075, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# XGB_model_3.fit(X_train_3,y_train_3)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(XGB_model_3.predict(X_valid_3), y_valid_3))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(XGB_model_3.predict(X_valid_3), y_valid_3)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(XGB_model_3.predict(X_valid_3), y_valid_3)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # Random Forest\n",
    "# print (\"Random-Forest Regressor:\")\n",
    "# RF_model_3 = RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# RF_model_3.fit(X_train_3, y_train_3)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(RF_model_3.predict(X_valid_3), y_valid_3))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(RF_model_3.predict(X_valid_3), y_valid_3)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(RF_model_3.predict(X_valid_3), y_valid_3)[0])\n",
    "# print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-training best model on best features\n",
    "test_model_3 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "test_model_3.fit(X_train_3, y_train_3)\n",
    "\n",
    "# Validation\n",
    "print (\"Validation:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_3.predict(X_valid_3), y_valid_3), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_3.predict(X_valid_3), y_valid_3)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_3.predict(X_valid_3), y_valid_3)[0], decimals=3))\n",
    "print ()\n",
    "\n",
    "# Testing the model\n",
    "print (\"Testing:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_3.predict(X_test_3), y_test_3), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_3.predict(X_test_3), y_test_3)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_3.predict(X_test_3), y_test_3)[0], decimals=3))\n",
    "\n",
    "# Saving the weights\n",
    "pickle.dump(test_model_3, open(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/model_coq_3.pkl\", \"wb\"))\n",
    "np.save(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/feature_indices_coq_3.npy\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Over Quality-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous Cross-Over Qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_COQ_3 = test_model_3.predict(X_train_3).reshape(-1,1)\n",
    "valid_COQ_3 = test_model_3.predict(X_valid_3).reshape(-1,1)\n",
    "test_COQ_3 = test_model_3.predict(X_test_3).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Validation and Test Datasets\n",
    "X_train_4, y_train_4 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=train_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(1280,720),\n",
    "\tlow_res=(960,540)\n",
    ")\n",
    "X_valid_4, y_valid_4 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=valid_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(1280,720),\n",
    "\tlow_res=(960,540)\n",
    ")\n",
    "X_test_4, y_test_4 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=test_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(1280,720),\n",
    "\tlow_res=(960,540)\n",
    ")\n",
    "\n",
    "# Concatenating Low-Level Features and Previous Cross-Over Qualities\n",
    "X_train_4 = np.concatenate([X_train_4, train_COQ_1, train_COQ_2, train_COQ_3], axis=1)\n",
    "X_valid_4 = np.concatenate([X_valid_4, valid_COQ_1, valid_COQ_2, valid_COQ_3], axis=1)\n",
    "X_test_4 = np.concatenate([X_test_4, test_COQ_1, test_COQ_2, test_COQ_3], axis=1)\n",
    "\n",
    "\n",
    "# Assertions\n",
    "assert (X_train_4.shape[1] == X_valid_4.shape[1] == X_test_4.shape[1]) and (y_train_4.shape[1] == y_valid_4.shape[1] == y_test_4.shape[1]), \"No.of features do not match.\"\n",
    "print (\"Dimensions of training dataset = \", X_train_4.shape, y_train_4.shape)\n",
    "print (\"Dimensions of validation dataset = \", X_valid_4.shape, y_valid_4.shape)\n",
    "print (\"Dimensions of test dataset = \", X_test_4.shape, y_test_4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "rfe_4 = RFE(estimator=RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\"), n_features_to_select=9)\n",
    "rfe_4.fit(X_train_4, y_train_4)\n",
    "indices = [i for i,v in enumerate(rfe_4.support_) if v]\n",
    "\n",
    "# Features Selected\n",
    "f = features_set + [\"COQ_1\", \"COQ_2\", \"COQ_3\"]\n",
    "print ([f[i] for i in indices])\n",
    "\n",
    "# Feature-Elimination on Inputs\n",
    "X_train_4 = X_train_4[...,indices]\n",
    "X_valid_4 = X_valid_4[...,indices]\n",
    "X_test_4 = X_test_4[...,indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extra-Trees Regressor\n",
    "# print (\"Extra-Trees Regressor:\")\n",
    "# ETR_model_4 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# ETR_model_4.fit(X_train_4, y_train_4)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(ETR_model_4.predict(X_valid_4), y_valid_4))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(ETR_model_4.predict(X_valid_4), y_valid_4)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(ETR_model_4.predict(X_valid_4), y_valid_4)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # XG-Boost\n",
    "# print (\"XG-Boost Regressor:\")\n",
    "# XGB_model_4 = XGBRegressor(n_estimators=1250, learning_rate=0.0075, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# XGB_model_4.fit(X_train_4,y_train_4)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(XGB_model_4.predict(X_valid_4), y_valid_4))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(XGB_model_4.predict(X_valid_4), y_valid_4)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(XGB_model_4.predict(X_valid_4), y_valid_4)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # Random Forest\n",
    "# print (\"Random-Forest Regressor:\")\n",
    "# RF_model_4 = RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# RF_model_4.fit(X_train_4, y_train_4)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(RF_model_4.predict(X_valid_4), y_valid_4))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(RF_model_4.predict(X_valid_4), y_valid_4)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(RF_model_4.predict(X_valid_4), y_valid_4)[0])\n",
    "# print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-training best model on best features\n",
    "test_model_4 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "test_model_4.fit(X_train_4, y_train_4)\n",
    "\n",
    "# Validation\n",
    "print (\"Validation:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_4.predict(X_valid_4), y_valid_4), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_4.predict(X_valid_4), y_valid_4)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_4.predict(X_valid_4), y_valid_4)[0], decimals=3))\n",
    "print ()\n",
    "\n",
    "# Testing the model\n",
    "print (\"Testing:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_4.predict(X_test_4), y_test_4), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_4.predict(X_test_4), y_test_4)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_4.predict(X_test_4), y_test_4)[0], decimals=3))\n",
    "\n",
    "# Saving the weights\n",
    "pickle.dump(test_model_4, open(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/model_coq_4.pkl\", \"wb\"))\n",
    "np.save(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/feature_indices_coq_4.npy\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Over Quality-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous Cross-Over Qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_COQ_4 = test_model_4.predict(X_train_4).reshape(-1,1)\n",
    "valid_COQ_4 = test_model_4.predict(X_valid_4).reshape(-1,1)\n",
    "test_COQ_4 = test_model_4.predict(X_test_4).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training, Validation and Test Datasets\n",
    "X_train_5, y_train_5 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=train_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(960,540),\n",
    "\tlow_res=(768,432)\n",
    ")\n",
    "X_valid_5, y_valid_5 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=valid_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(960,540),\n",
    "\tlow_res=(768,432)\n",
    ")\n",
    "X_test_5, y_test_5 = dataset_evaluation_functions.LowLevelFeatures_CrossOverQualities_Dataset(\n",
    "\tcodec=\"libx265\",\n",
    "\tpreset=\"medium\",\n",
    "\tquality_metric=\"vmaf\",\n",
    "\tfeatures_names=features_set,\n",
    "\tvideo_filenames=test_video_filenames,\n",
    "\ttemporal_low_level_features=False,\n",
    "\tResolutions_Considered=Resolutions,\n",
    "\tCRFs_Considered=CRFs,\n",
    "\tbitrates_Considered=None,\n",
    "\tQPs_Considered=None,\n",
    "\thigh_res=(960,540),\n",
    "\tlow_res=(768,432)\n",
    ")\n",
    "\n",
    "# Concatenating Low-Level Features and Previous Cross-Over Qualities\n",
    "X_train_5 = np.concatenate([X_train_5, train_COQ_1, train_COQ_2, train_COQ_3, train_COQ_4], axis=1)\n",
    "X_valid_5 = np.concatenate([X_valid_5, valid_COQ_1, valid_COQ_2, valid_COQ_3, valid_COQ_4], axis=1)\n",
    "X_test_5 = np.concatenate([X_test_5, test_COQ_1, test_COQ_2, test_COQ_3, test_COQ_4], axis=1)\n",
    "\n",
    "\n",
    "# Assertions\n",
    "assert (X_train_5.shape[1] == X_valid_5.shape[1] == X_test_5.shape[1]) and (y_train_5.shape[1] == y_valid_5.shape[1] == y_test_5.shape[1]), \"No.of features do not match.\"\n",
    "print (\"Dimensions of training dataset = \", X_train_5.shape, y_train_5.shape)\n",
    "print (\"Dimensions of validation dataset = \", X_valid_5.shape, y_valid_5.shape)\n",
    "print (\"Dimensions of test dataset = \", X_test_5.shape, y_test_5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "rfe_5 = RFE(estimator=RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\"), n_features_to_select=9)\n",
    "rfe_5.fit(X_train_5, y_train_5)\n",
    "indices = [i for i,v in enumerate(rfe_5.support_) if v]\n",
    "\n",
    "# Features Selected\n",
    "f = features_set + [\"COQ_1\", \"COQ_2\", \"COQ_3\", \"COQ_4\"]\n",
    "print ([f[i] for i in indices])\n",
    "\n",
    "# Feature-Elimination on Inputs\n",
    "X_train_5 = X_train_5[...,indices]\n",
    "X_valid_5 = X_valid_5[...,indices]\n",
    "X_test_5 = X_test_5[...,indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extra-Trees Regressor\n",
    "# print (\"Extra-Trees Regressor:\")\n",
    "# ETR_model_5 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# ETR_model_5.fit(X_train_5, y_train_5)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(ETR_model_5.predict(X_valid_5), y_valid_5))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(ETR_model_5.predict(X_valid_5), y_valid_5)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(ETR_model_5.predict(X_valid_5), y_valid_5)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # XG-Boost\n",
    "# print (\"XG-Boost Regressor:\")\n",
    "# XGB_model_5 = XGBRegressor(n_estimators=1250, learning_rate=0.0075, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# XGB_model_5.fit(X_train_5,y_train_5)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(XGB_model_5.predict(X_valid_5), y_valid_5))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(XGB_model_5.predict(X_valid_5), y_valid_5)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(XGB_model_5.predict(X_valid_5), y_valid_5)[0])\n",
    "# print ()\n",
    "\n",
    "\n",
    "# # Random Forest\n",
    "# print (\"Random-Forest Regressor:\")\n",
    "# RF_model_5 = RandomForestRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "# RF_model_5.fit(X_train_5, y_train_5)\n",
    "\n",
    "# print (\"MSE =\", sklearn.metrics.mean_squared_error(RF_model_5.predict(X_valid_5), y_valid_5))\n",
    "# print (\"PLCC =\", scipy.stats.pearsonr(RF_model_5.predict(X_valid_5), y_valid_5)[0][0])\n",
    "# print (\"SRCC =\", scipy.stats.spearmanr(RF_model_5.predict(X_valid_5), y_valid_5)[0])\n",
    "# print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-training best model on best features\n",
    "test_model_5 = ExtraTreesRegressor(n_estimators=1250, criterion=\"squared_error\", random_state=2, max_depth=12, max_features=\"log2\")\n",
    "test_model_5.fit(X_train_5, y_train_5)\n",
    "\n",
    "# Validation\n",
    "print (\"Validation:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_5.predict(X_valid_5), y_valid_5), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_5.predict(X_valid_5), y_valid_5)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_5.predict(X_valid_5), y_valid_5)[0], decimals=3))\n",
    "print ()\n",
    "\n",
    "# Testing the model\n",
    "print (\"Testing:\\n\")\n",
    "print (\"MSE =\", np.round(sklearn.metrics.mean_squared_error(test_model_5.predict(X_test_5), y_test_5), decimals=3))\n",
    "print (\"PLCC =\", np.round(scipy.stats.pearsonr(test_model_5.predict(X_test_5), y_test_5)[0][0], decimals=3))\n",
    "print (\"SRCC =\", np.round(scipy.stats.spearmanr(test_model_5.predict(X_test_5), y_test_5)[0], decimals=3))\n",
    "\n",
    "# Saving the weights\n",
    "pickle.dump(test_model_5, open(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/model_coq_5.pkl\", \"wb\"))\n",
    "np.save(\"/home/krishna/Constructing-Per-Shot-Bitrate-Ladders-using-Visual-Information-Fidelity/quality_ladder_construction/models/ML/CrossOver_Qualities/low_level_features/feature_indices_coq_5.npy\", indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualEnv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
